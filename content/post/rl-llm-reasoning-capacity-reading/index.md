---
title: "論文閱讀：Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?"
summary: "探討強化學習與可驗證獎勵（RLVR）在提升 LLM 推理能力上的真實效果"
date: 2025-11-22
authors:
  - admin
tags:
  - 論文閱讀
  - Large Language Models
  - Reinforcement Learning
  - 推理能力
  - RLVR
draft: false
---

# 論文資訊

**標題**: Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?  
**來源**: arXiv  
**論文 ID**: 2504.13837  
**連結**: [arXiv](https://arxiv.org/abs/2504.13837) | [PDF](https://arxiv.org/pdf/2504.13837)

---

## 研究背景

強化學習與可驗證獎勵（Reinforcement Learning with Verifiable Rewards, RLVR）近期在提升大型語言模型的推理表現上取得了顯著成功，特別是在數學和程式設計任務上。

傳統觀點認為，RLVR 能夠使 LLMs 持續自我改進，從而獲得超越基礎模型的新型推理能力——就像傳統 RL 幫助智能體探索和學習新策略一樣。

但是這個假設真的成立嗎？

本研究通過系統性地探測 RLVR 訓練的 LLMs 在各種場景下的推理能力邊界，試圖回答一個關鍵問題：

**RLVR 訓練是否真的讓 LLMs 獲得了超越基礎模型的新推理能力？**

---

## 研究方法

### 評估設計
研究團隊使用 **pass@k（在大 k 值時）** 作為評估指標，跨越多個維度：
- **模型家族**：多個不同的 LLM 架構
- **RL 演算法**：六種流行的 RLVR 演算法
- **基準測試**：數學、程式設計、視覺推理

### 分析方法
1. **Pass@k 評估**：使用大 k 值來探測模型的能力上界
2. **覆蓋率分析**（Coverage Analysis）：衡量 RLVR 訓練模型能否產生基礎模型無法產生的解決方案
3. **困惑度分析**（Perplexity Analysis）：評估 RLVR 訓練模型的輸出是否仍在基礎模型的分佈內

---

## 主要發現

### 1. 當前訓練設置無法激發根本性的新推理模式

令人驚訝的核心發現：

- ✅ **小 k 值時（如 k=1）**: RLVR 訓練模型**優於**基礎模型
- ❌ **大 k 值時**: 基礎模型反而獲得**更高**的 pass@k 分數

**這意味著什麼？**

RLVR 訓練主要是在「篩選」（Selecting）基礎模型已有的能力，而非創造新的推理模式。就像是從一個已經裝滿了各種工具的工具箱裡，學會更快地挑出正確的工具，但並沒有製造出新的工具。

### 2. 推理能力受限於基礎模型

通過覆蓋率分析和困惑度分析，研究發現：

- 觀察到的推理能力**源自**並**受限於**基礎模型
- 將基礎模型視為能力上界，六種流行的 RLVR 演算法表現相似
- 所有方法都遠未達到充分利用基礎模型潛力的最優狀態

**基礎模型就是天花板**——無論如何訓練，RLVR 都無法讓模型跳出基礎模型定義的能力邊界。

### 3. 蒸餾方法展現不同的優勢

相比之下，**知識蒸餾**（Distillation）表現出不同的特性：

- 能夠從教師模型引入**新的推理模式**
- **真正擴展**模型的推理能力
- 不只是重新排列現有知識

這暗示了一個重要的方法論差異：蒸餾能夠「教會」模型新的推理路徑，而 RLVR 只是在「優化選擇」。

---

## 研究意義與啟示

### 對 RL 在 LLM 領域的反思

這項研究揭示了當前 RLVR 方法的一個根本性限制：

> **當前的 RLVR 訓練範式尚未實現 RL 在 LLMs 中激發真正新穎推理能力的潛力。**

這與我們對 RL 的期待有很大差距。在傳統 RL 領域（如遊戲 AI），RL 能夠發現人類從未想過的策略。但在 LLM 領域，目前的 RLVR 方法似乎只是在「挖掘」而非「創造」。

### 未來研究方向

研究強調了改進 RL 範式的必要性：

1. **持續擴展**（Continual Scaling）
   - 長期、持續的訓練過程
   - 而非一次性的優化

2. **多輪代理-環境互動**（Multi-turn Agent-Environment Interaction）
   - 真正的互動式學習
   - 而非靜態數據集上的訓練

3. **新的評估範式**
   - Pass@k（大 k）而非只看 pass@1
   - 更全面地評估能力邊界

---

## 我的理解

讀完這篇論文，我最大的感受是：這真的是一篇「打臉」論文。過去這一年，RLVR 在 LLM 領域被捧得很高，OpenAI 的 o1、DeepSeek 的 R1 都號稱用了強化學習讓模型「學會思考」。但這篇研究冷靜地告訴我們：慢著，你確定模型真的學會了新東西嗎？還是只是學會了從已有的工具箱裡更快地挑出正確答案？

### 1. 「選擇器」vs「創造器」：一個殘酷的真相

我覺得這篇論文最核心的洞察，就是把 RLVR 定位為「選擇器」而非「能力擴展器」。這聽起來可能有點抽象，但其實很好理解。想像你有一個學生，他的腦子裡已經有很多解題方法了（基礎模型），但他不太確定哪個方法最好，所以隨機試。RLVR 訓練就像是給他做了很多練習題，讓他學會「看到這種題就用這個方法」，提高了他的答題準確率（pass@1）。

但問題是，如果你給這個學生很多次嘗試的機會（pass@k，大 k），他本來就能靠「試錯」把正確答案試出來。RLVR 訓練並沒有教會他任何新的解題方法，只是讓他更快地找到對的那個。這就是為什麼在 pass@k 大的時候，基礎模型反而表現更好——因為它還保有更多元的嘗試可能性，而 RLVR 訓練後的模型已經被「定型」了，只會優先選擇訓練過程中獎勵高的那些路徑。

這個發現其實很殘酷，因為它意味著我們過去一年看到的很多「推理能力提升」，可能只是表面功夫。

### 2. 基礎模型是天花板：投資應該放在哪？

這篇論文讓我重新思考資源分配的問題。如果基礎模型就是能力的天花板，那產業界是不是應該把更多資源投入在 pre-training 上，而不是在 RLVR 的各種花式調參上？

目前的趨勢是，大家拼命在想怎麼用更少的數據、更巧妙的 reward shaping 來做 RLVR。但這篇研究告訴我們，再怎麼優化，你也跳不出基礎模型的能力邊界。那與其在 RLVR 上精雕細琢，不如直接訓練一個更強的基礎模型豈不是更實在？

當然，這不是說 RLVR 完全沒用。在 pass@1 的場景下（也就是實際應用中最常見的情況），RLVR 確實能顯著提升表現。但我們得清楚知道它的本質：它是一個「優化器」而非「擴展器」。

### 3. 知識蒸餾的啟示：為什麼它能做到 RLVR 做不到的事？

論文提到知識蒸餾能引入新的推理模式，這讓我很好奇。為什麼蒸餾可以，而 RLVR 不行？

我的理解是，蒸餾本質上是「知識遷移」——你從一個更強的老師模型那裡，學到了「新的思考方式」。這是真正的能力擴展。而 RLVR 只是在自己的能力範圍內「自我優化」，沒有外部知識的注入。

這讓我想到，也許未來真正有效的訓練範式，應該是「蒸餾 + RLVR」的結合：先用蒸餾擴展能力邊界，再用 RLVR 優化決策品質。單靠 RLVR 想讓模型「自己涌現」出新能力，可能本來就是一個不切實際的期待。

### 4. Pass@k 的哲學：我們到底在評估什麼？

這篇論文也提醒了我評估方法的重要性。我們習慣用 pass@1 來評估模型，因為它最接近真實應用場景。但 pass@1 可能給我們一個過度樂觀的假象。

Pass@k（大 k）揭示的是模型的「能力邊界」——它「可能」做對的上限。而 pass@1 只是告訴我們模型「通常」會怎麼做。這兩者的gap，就是 RLVR 在做的事情：縮小「可能」和「通常」之間的距離。

但如果我們只看 pass@1，就會誤以為 RLVR 真的讓模型變聰明了。實際上，模型的「天花板」並沒有提高，只是「平均表現」提高了。這個區別很重要。

### 5. 對未來的思考：真正的推理需要什麼？

讀完這篇論文，我覺得當前的 RLVR 範式可能從一開始就設計錯了。我們把 LLM 當成一個「靜態的策略空間」，然後用 RL 去優化策略選擇。但真正的推理，不應該是這樣的。

真正的推理應該是「動態的、互動的、持續學習的」。就像人類不是通過做題練習學會思考，而是通過與世界的長期互動、試錯、反思來發展推理能力。論文提到的「多輪代理-環境互動」可能才是正確的方向。

也許我們需要的不是用 RL 來訓練模型，而是用 RL 來讓模型「活」在一個環境裡，真正地探索、犯錯、學習、成長。那才是 RL 的本質，而不是現在這種在靜態數據集上做的「偽 RL」。

### 6. 一個令人不安的問題

最後，這篇論文讓我想到一個更深層的問題：如果 RLVR 無法讓模型真正變聰明，那 o1 這些號稱有「推理能力」的模型，到底是怎麼回事？

有兩種可能：一是它們的基礎模型本來就非常強大，RLVR 只是把這個潛力發掘出來了；二是它們用了一些這篇論文沒有涵蓋的技巧（比如 chain-of-thought、test-time compute scaling）。

無論如何，這篇研究提醒我們：不要被表面的性能提升迷惑，要深入思考能力的本質來源。這對做研究和做產品都很重要。

---

## 延伸思考

### 1. RLVR vs. 知識蒸餾：本質差異是什麼？

- 為什麼蒸餾能引入新模式？
- 兩者在訓練過程中的根本區別在哪裡？
- 能否結合兩者的優勢？

### 2. 評估方法的重要性

- Pass@1 vs. Pass@k（大 k）的差異揭示了什麼？
- 我們是否過度依賴單一指標來評估模型能力？
- 還有哪些評估維度可以探索？

### 3. 基礎模型的角色

- 投資於更好的基礎模型訓練 vs. 更複雜的 RLVR 方法，哪個更值得？
- Pre-training 的改進方向在哪裡？
- 基礎模型的「能力邊界」是如何形成的？

---

## 相關資源

- [arXiv 論文頁面](https://arxiv.org/abs/2504.13837)
- [PDF 下載](https://arxiv.org/pdf/2504.13837)

---

**閱讀日期**: 2025-11-22  
**筆記整理**: Dylan Chiang
